\documentclass[10pt,handout,english]{beamer}
\usetheme{Warsaw}
\usepackage{graphicx}
\usepackage{placeins}
\usepackage{amsmath, amssymb}
\usepackage{tabu}
\usepackage{bbm}
\usepackage{booktabs}
\usepackage[round]{natbib}
\usepackage{bm}
\usepackage{ragged2e}
\usepackage{bbm}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{xcolor}
\usepackage[super]{nth}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=blue,      
    urlcolor=blue,
    citecolor=black,
}

\apptocmd{\frame}{}{\justifying}{} % Allow optional arguments after frame.

\setbeamertemplate{frametitle continuation}{}

\newcommand\setItemnumber[1]{\setcounter{enumi}{\numexpr#1-1\relax}}

\DeclareMathOperator{\tr}{tr}

\newcommand{\ts}{\textsuperscript}
\newcommand{\E}{\mathbb{E}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\F}{\mathcal{F}}
\newcommand{\T}{\mathbb{T}}
\newcommand{\C}{\mathbb{C}}


\newcommand{\vertiii}[1]{{\left\vert\kern-0.25ex\left\vert\kern-0.25ex\left\vert #1 
    \right\vert\kern-0.25ex\right\vert\kern-0.25ex\right\vert}}


\title[]{Bounds on $l_2$-error for hard sparse models}
\author[Kaveh S. Nobari]{Kaveh S. Nobari}
\institute[]{Lectures in High-Dimensional Statistics}
\date[27/10/2020]
{Department of Mathematics and Statistics\\ Lancaster University}
	

\begin{document}
\begin{frame}
\titlepage
\end{frame}


\begin{frame}{Contents}
\tableofcontents
\end{frame}


\section{Recap}
\frame{\tableofcontents[currentsection,currentsubsection]}

\begin{frame}[allowframebreaks]
\begin{itemize}
\setlength\itemsep{0.5em}
\item Consider the vector matrix pair $(y,X)\in\R^n\times \R^{n\times d}$ linked by the linear model
\[
y=X\theta^*+\varepsilon
\]
where $\varepsilon \in \R^n$ is the noise vector.
\item Aim is to ensure $\lVert \hat{\theta}-\theta^*\rVert_2^2$ is small.\justifying
\item The latter requires conditions on the random design matrix $X$.\justifying
\item Further, the choice of $(n,d,s)$ is important in ensuring that $\lVert \hat{\theta}-\theta^*\rVert_2^2$ is small.\justifying 
\item In the presence of noise, a natural extension to the basis pursuit program was accomplished by minimizing a weighted combination of the data-fidelity  term $\lVert y-X\beta\rVert_2^2$ with the $L_1$-norm penalty:\justifying
\begin{equation}\label{eq: Lagrangian Lasso}
\hat{\theta}\in\arg\min_{\theta \in \R^d}\left\{\frac{1}{2n}\lVert y-X\theta\rVert_2^2+\lambda_n\lVert\theta\rVert_1\right\},\quad\text{\textcolor{red}{(Lagrangian Lasso)}}
\end{equation} 
or equivalently, the constrained forms of Lasso
\begingroup
\allowdisplaybreaks
\begin{align}
\min&_{\theta\in\R^d}\left\{\frac{1}{2n}\lVert y-X\theta\rVert_2^2\right\},\quad\text{s.t.}\quad \lVert\theta\rVert_1\leq R,\quad\text{\textcolor{red}{(Constrained Lasso)}}\label{eq: Constrained Lasso}\\
\min&_{\theta\in\R^d}\lVert\theta\rVert_1\quad\text{s.t.}\quad\frac{1}{2n}\lVert y-X\theta\rVert_2^2\leq b^2,\quad\text{\textcolor{red}{(Relaxed BP)}}\label{eq: RBP}
\end{align}
\endgroup
\item Let $\Delta:=\hat{\theta}-\theta^*$. The goal is thus to bound $\lVert\Delta\rVert_2^2$.\justifying 
\item In low dimensions, a bound on $\lVert X\Delta\rVert_2^2$ would provide guarantees for $\lVert\Delta\rVert_2^2$.\justifying
\item This, however, no longer holds true in high dimensions, as $X$ has a non-trivial nullspace.\justifying
\item Perfect recovery \textcolor{red}{not feasible in noisy settings}.\justifying
\item We thus focus on bounding the $L_2$ error $\lVert \hat{\theta}-\theta^*\rVert_2$. \justifying
\item In noisy settings, the required condition is slightly stronger than the Restricted Nullspace Property - namely that the restricted eigenvalues of the matrix $\frac{X'X}{n}$ are lower bounded over a cone.\justifying
\item In particular, for a constant $\alpha\geq 1$, let us define the set
\[
\C_{\alpha}(S):=\{\Delta\in\R^d\mid\lVert\Delta_{S^c}\rVert_1\leq\alpha\lVert\Delta_S\rVert_1\}
\]
We then say, the matrix $X$ satisfies the \textcolor{red}{Restricted Eigenvalue Condition} over $S$ with parameters $(\kappa,\alpha)$, if
\[
\frac{1}{n}\lVert X\Delta \rVert_2^2\geq \kappa\lVert \Delta\rVert_2^2,\quad\forall\Delta\in\C_{\alpha}(S)
\]
\end{itemize}
\end{frame}


\section{Preliminaries}
\frame{\tableofcontents[currentsection,currentsubsection]}
\begin{frame}[allowframebreaks]
\begin{block}{Holder's inequality}
If $p>1$ and $\frac{1}{p}+\frac{1}{q}=1$ and $\E\left(\lvert Y\rvert^p\right)\leq\infty$ and $\E\left(\lvert Z\rvert^q\right)\leq\infty$ then
\[
\E\left(\lvert YZ\rvert\right)\leq \left[\E\left(\lvert Y\rvert^p\right)\right]^{1/p}\left[\E\left(\lvert Z\rvert^q\right)\right]^{1/q}
\]
[See \citet{white2014asymptotic}]
\end{block}
A special case of the Holder's inequality is the Cauchy-Schwarz inequality as follows:
\begin{block}{Cauchy-Schwarz inequality}
If $p>1$ and $\frac{1}{p}+\frac{1}{q}=1$, such that $p=q=2$ and $\E\left(\lvert Y\rvert^2\right)\leq\infty$ and $\E\left(\lvert Z\rvert^2\right)\leq\infty$ then
\[
\E\left(\lvert YZ\rvert\right)\leq \left[\E\left(\lvert Y\rvert^2\right)\right]^{1/2}\left[\E\left(\lvert Z\rvert^2\right)\right]^{1/2}
\]
[See \citet{white2014asymptotic}]
\end{block}
\begin{example}
For $u\in\R^s$, prove
\[
\lVert u\rVert_1\leq \sqrt{s}\lVert u\rVert_2
 \]

\noindent\textbf{Solution:}
\begin{align*}
\lVert u\rVert_1&=\sum\limits_{i=1}^s\lvert u_i\rvert\\
&=\sum\limits_{i=1}^s\lvert u_i.1\rvert\\
&\leq\left(\sum\limits_{i=1}^s\lvert u_i\rvert^2\right)^{1/2}\left(\sum\limits_{i=1}^s1\right)^{1/2}\\
&=\sqrt{s}\lVert u\rVert_2
\end{align*}
\end{example}
\end{frame}
\section{$L_2$ error between $\hat{\theta}$ and $\theta^*$}
\frame{\tableofcontents[currentsection,currentsubsection]}

\begin{frame}[allowframebreaks]
Following \citet{wainwright2019high}, we now state a result that provides a bound on the error $\lVert \hat{\theta}-\theta^*\rVert_2$ in the case of a \textcolor{red}{hard sparse} vector $\theta^*$. \justifying

Let us impose the following conditions:
\begin{itemize}
\setlength\itemsep{0.5em}
\item[A1.] The vector $\theta^*$ is supported on a subset $S\subseteq \{1,2,\cdots,d\}$ with $\lvert S\rvert=s$. 
\item[A2.] The design matrix satisfies the Restricted Eigenvalue condition over $S$ with parameters $(\kappa,3)$.
\end{itemize}
\begin{theorem}[$L_2$-error between $\hat{\theta}$(\ref{eq: Lagrangian Lasso}) and $\theta^*$]
Any solution to the Lagrangian Lasso (\ref{eq: Lagrangian Lasso}) with regularization parameter lower bounded as $\lambda_n\geq2\lVert\frac{X'\varepsilon}{n}\rVert_{\infty}$, satisfies the bound
\[
\lVert \hat{\theta}-\theta^*\rVert_2\leq\frac{3}{\kappa}\sqrt{s}\lambda_n
\]
\end{theorem}
\end{frame}
\begin{frame}[allowframebreaks]
\noindent\textbf{Proof:} Recall from earlier that $\lVert \hat{\theta} \rVert_1\leq \lVert\theta^*\rVert_1$. Thus, from (\ref{eq: Lagrangian Lasso}), we get
\[
\frac{1}{2n}\lVert y-X\hat{\theta}\rVert_2^2+\lambda_n\lVert\hat{\theta}\rVert_1\leq \frac{1}{2n}\lVert y-X\theta^*\rVert_2^2+\lambda_n\lVert\theta^*\rVert_1
\]
Note from earlier that $\Delta:=\hat{\theta}-\theta^*$, and furthermore, for two column vectors $u$ and $v$, we can expand 
\[
\lVert u-v\rVert_2^2=\lVert u\rVert_2^2-2u'v+\lVert v\rVert_2^2
\]
Thus,
\begin{align*}
\frac{1}{2n}\lVert y\rVert_2^2-\frac{1}{n}(X\hat{\theta})'y+\frac{1}{2n}\lVert X\hat{\theta}\rVert_2^2+\lambda_n\lVert\hat{\theta}\rVert_1\leq\\
\frac{1}{2n}\lVert y\rVert_2^2-\frac{1}{n}(X\theta^*)'y+\frac{1}{2n}\lVert X\theta^*\rVert_2^2+\lambda_n\lVert\theta^*\rVert_1
\end{align*}
which simplifies to
\begin{equation}\label{eq: proof1}
\frac{1}{n}\lVert X\Delta\rVert_2^2\leq\frac{2(X'\varepsilon)'\Delta}{n}+2\lambda_n\{\lVert\theta^*\rVert_1-\lVert\hat{\theta}\rVert_1\}
\end{equation}
Expressing $\theta^*$ under $s$-sparse condition, we get
\begingroup
\allowdisplaybreaks
\begin{align*}
\lVert\theta^*\rVert_1-\lVert\hat{\theta}\rVert_1&=\lVert\theta_S^*+\theta_{S^c}^*\rVert_1-\lVert\theta^*+\Delta\rVert_1\\
&=\lVert\theta_S^*+\theta_{S^c}^*\rVert_1-\lVert\theta_S^*+\theta_{S^c}^*+\Delta_S+\Delta_{S^c}\rVert_1\\
&=\lVert\theta_S^*\rVert_1-\lVert\theta_S^*+\Delta_S+\Delta_{S^c}\rVert_1\\
&=\lVert\theta_S^*\rVert_1-\lVert\theta_S^*+\Delta_S\rVert_1-\lVert\Delta_{S^c}\rVert_1
\end{align*}
\endgroup
Substituting the above expression in (\ref{eq: proof1}), we obtain 
\begin{equation}\label{eq: proof2}
\frac{1}{n}\lVert X\Delta\rVert\leq\frac{2(X'\varepsilon)'\Delta}{n}+2\lambda_n\{\lVert\theta_S^*\rVert_1-\lVert\theta_S^*+\Delta_S\rVert_1-\lVert\Delta_{S^c}\rVert_1\}
\end{equation}
Using Holder's inequality we have
\[
\frac{2(X'\varepsilon)'\Delta}{n}\leq \left\lVert\frac{2(X'\varepsilon)'}{n}\right\rVert_{\infty}\lVert\Delta\rVert_1
\]
which upon substitution in (\ref{eq: proof2}) yields
\begin{equation}\label{eq: proof3}
\frac{1}{n}\lVert X\Delta\rVert\leq\left\lVert\frac{2(X'\varepsilon)'}{n}\right\rVert_{\infty}\lVert\Delta\rVert_1+2\lambda_n\{\lVert\theta_S^*\rVert_1-\lVert\theta_S^*+\Delta_S\rVert_1-\lVert\Delta_{S^c}\rVert_1\}
\end{equation}

Furthermore, from the triangle inequality we have
\[
\lVert\theta_S^*+\Delta_S\rVert_1\leq \lVert\theta_S^*\rVert_1+\lVert\Delta_S\rVert_1 
\]

thus,
\begin{equation}\label{eq: proof4}
\lVert\theta_S^*\rVert_1-\lVert\theta_S^*+\Delta_S\rVert_1-\lVert\Delta_{S^c}\rVert_1\leq \lVert\theta_S^*\rVert_1-\lVert\theta_S^*\rVert_1+\lVert\Delta_S\rVert_1-\lVert\Delta_{S^c}\rVert_1
\end{equation}

where plugging in (\ref{eq: proof4}) into (\ref{eq: proof3}) yields
\begin{equation}\label{eq: proof5}
\frac{1}{n}\lVert X\Delta\rVert\leq2\left\lVert\frac{(X'\varepsilon)'}{n}\right\rVert_{\infty}\lVert\Delta\rVert_1+2\lambda_n\{\lVert\Delta_S\rVert_1-\lVert\Delta_{S^c}\rVert_1\}.
\end{equation}

Recall the condition $\lambda_n\geq 2\left\lVert\frac{X'\varepsilon}{n}\right\rVert_{\infty}$. Substituting the choice of $\lambda_n$ into equation (\ref{eq: proof5}) yields

\begin{equation}\label{eq: proof6}
\frac{1}{n}\lVert X\Delta\rVert\leq \lambda_n\{3\lVert\Delta_S\rVert_1-\lVert\Delta_{S^c}\rVert_1\}
\end{equation}

Inequality (\ref{eq: proof6}) shows that $\Delta\in\C_3(S)$ - i.e. since
\[
0\leq\frac{1}{n}\lVert X\Delta\rVert\leq\lambda_n\{3\lVert\Delta_S\rVert_1-\lVert\Delta_{S^c}\rVert_1\}
\]

then $\lVert\Delta_{S^c}\rVert_1\leq 3\lVert\Delta_S\rVert_1$, so that the Restricted Eigenvalue condition may be applied. In other words,
\[
\kappa\lVert\Delta\rVert_2^2\leq 3\lambda_n\sqrt{s}\lVert\Delta\rVert_2,
\]
which implies the Theorem.
\end{frame}
\begin{frame}[allowframebreaks]
\begin{theorem}[$L_2$-error between $\hat{\theta}$(\ref{eq: Constrained Lasso}) and $\theta^*$]
Any solution of the Constrained Lasso (\ref{eq: Constrained Lasso}) with $R=\lVert\theta^*\rVert_1$ satisfies the bound
\[
\lVert \hat{\theta}-\theta^*\rVert_2\leq\frac{4}{\kappa}\sqrt{s}\left\lVert \frac{X'\varepsilon}{n}\right\rVert_{\infty}
\]
\end{theorem}
\noindent\textbf{Proof:} Given the choice of $t=\lVert \theta^*\rVert_1$, the target vector $\theta^*$ is feasible. Since $\hat{\theta}$ is optimal, we have the inequality
\[
\frac{1}{2n}\lVert y-X\hat{\theta}\rVert_2^2\leq\frac{1}{2n}\lVert y-X\theta^*\rVert_2^2.
\]
As before, define the vector $\Delta:=\hat{\theta}-\theta^*$. As in the proof of the previous Theorem, we can easily show
\begin{equation}\label{eq: 1proof}
\frac{1}{n}\lVert X\Delta\rVert_2^2\leq\frac{2(X'\varepsilon)'\Delta}{n}
\end{equation}
Once again, we know from Holder's inequality that
\[
\frac{2(X'\varepsilon)'\Delta}{n}\leq2\left\lVert\frac{(X'\varepsilon)'}{n}\right\rVert_{\infty}\lVert\Delta\rVert_1
\]
which when plugged into (\ref{eq: 1proof}), yields
\begin{equation}\label{eq: 2proof}
\frac{1}{n}\lVert X\Delta\rVert_2^2\leq2\left\lVert\frac{(X'\varepsilon)'}{n}\right\rVert_{\infty}\lVert\Delta\rVert_1
\end{equation}
On the other hand, following the proof of the analysis of basis pursuit in earlier sessions we obtain 
\[
\lVert\Delta_{S^c}\rVert_1\leq \lVert\Delta_{S}\rVert_1
\]
under our constraint on $\theta$ and the error $\Delta\in\C_1(S)$, whence
\[
\lVert\Delta\rVert_1=\lVert\Delta_S\rVert_1+\lVert\Delta_{S^c}\rVert_1\leq 2\lVert\Delta_S\rVert_1 \leq
2\sqrt{s}\lVert\Delta\rVert_2
\]
Because $\C_1(S)\subset\C_3(S)$, we may apply the Restricted Eigenvalue condition to the left hand side of (\ref{eq: 2proof}) and obtain 
\[
\frac{\lVert X\Delta\rVert_2^2}{n}\geq\kappa\lVert\Delta\rVert_2^2
\] 
Putting together all the pieces yields the bound of the Theorem.
\end{frame}

\begin{frame}[allowframebreaks]
\begin{theorem}[$L_2$-error between $\hat{\theta}$(\ref{eq: RBP}) and $\theta^*$]
Any solution to the Relaxed Basis Pursuit program (\ref{eq: RBP}) with $b^2\geq \frac{\lVert\varepsilon\rVert_2^2}{2n}$, satisfies the bound
\[
\lVert \hat{\theta}-\theta^*\rVert_2\leq\frac{4}{\kappa}\sqrt{s}\left\lVert\frac{X'\varepsilon}{n}\right\rVert+\frac{2}{\sqrt{\kappa}}\sqrt{b^2- \frac{\lVert\varepsilon\rVert_2^2}{2n}}
\]
\end{theorem}
\noindent\textbf{Proof:} Note that
\[
\frac{1}{2n}\lVert y-X\theta^*\rVert_2^2=\frac{\lVert\varepsilon\rVert_2^2}{2n}\leq b^2
\]
where the inequality on the right hand side of the above equation is the consequence of the assumed choice of $b$. Thus, the target vector $\theta^*$ is feasible, and since $\hat{\theta}$ is optimal, we have $\lVert\hat{\theta}\rVert_1\leq\lVert\theta^*\rVert_1$. As previously reasoned, the error vector $\Delta:=\hat{\theta}-\theta^*$ must then belong to the cone $\C_1(S)$. Now by the feasibility of $\hat{\theta}$, we have
\[
\frac{1}{2n}\lVert y-X\hat{\theta}\rVert_2^2\leq b^2=\frac{1}{2n}\lVert y-X\theta^*\rVert_2^2+\left(b^2-\frac{ \lVert\varepsilon\rVert_2^2}{2n} \right)
\]
where rearranging as in the proofs of previous Theorems yields the modified inequality
\[
\frac{\lVert X\Delta\rVert_2^2}{n}\leq2\frac{(X'\varepsilon)'\Delta}{n}+2\left(b^2-\frac{\lVert\varepsilon\rVert_2^2}{2n}\right)
\]
As in the previous Theorem, we may apply to Holder's inequality on the right hand side of the equation to yield
\[
2\frac{\varepsilon'X\Delta}{n}\leq2\left\lVert\frac{\varepsilon'X}{n}\right\rVert_{\infty}\lVert \Delta\rVert_1
\]
we obtain
\[
\frac{\lVert X\Delta\rVert_2^2}{n}\leq2\left\lVert\frac{\varepsilon'X}{n}\right\rVert_{\infty}\lVert \Delta\rVert_1+2\left(b^2-\frac{\lVert\varepsilon\rVert_2^2}{2n}\right)
\]
Moreover, we know that
\[
\lVert\Delta\rVert_1=\lVert\Delta_S\rVert_1+\lVert\Delta_{S^c}\rVert_1\leq 2\lVert\Delta_S\rVert_1\leq2\sqrt{s}\lVert\Delta\rVert_2
\]
Therefore,
\[
\frac{\lVert X\Delta\rVert_2^2}{n}\leq4\sqrt{s}\left\lVert\frac{\varepsilon'X}{n}\right\rVert_{\infty}\lVert \Delta\rVert_2+2\left(b^2-\frac{\lVert\varepsilon\rVert_2^2}{2n}\right)
\]
Applying the Restricted Eigenvalue condition as in the previous Theorem to the left hand side leads to
\[
\kappa\lVert\Delta\rVert_2^2\leq4\sqrt{s}\left\lVert\frac{\varepsilon'X}{n}\right\rVert_{\infty}\lVert \Delta\rVert_2+2\left(b^2-\frac{\lVert\varepsilon\rVert_2^2}{2n}\right)
\]
which implies that
\[
\lVert\Delta\rVert_2\leq\frac{8}{\kappa}\sqrt{s}\left\lVert\frac{X'\varepsilon}{n}\right\rVert_{\infty}+\frac{2}{\sqrt{\kappa}}\sqrt{b^2-\frac{\lVert\varepsilon\rVert_2^2}{2n}}
\]
as claimed.
\end{frame}

\begin{frame}[allowframebreaks]
\frametitle{References}
\bibliographystyle{apa}
\bibliography{References_HDStat}
\end{frame}
\end{document}