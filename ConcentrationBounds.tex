\documentclass[10pt,handout,english]{beamer}
\usetheme{Warsaw}
\usepackage{graphicx}
\usepackage{placeins}
\usepackage{tabu}
\usepackage{bbm}
\usepackage{booktabs}
\usepackage[round]{natbib}
\usepackage{bm}
\usepackage{ragged2e}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{xcolor}
\usepackage[super]{nth}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=blue,      
    urlcolor=blue,
    citecolor=black,
}

\apptocmd{\frame}{}{\justifying}{} % Allow optional arguments after frame.

\setbeamertemplate{frametitle continuation}{}

\newcommand\setItemnumber[1]{\setcounter{enumi}{\numexpr#1-1\relax}}

\newcommand{\ts}{\textsuperscript}
\newcommand{\E}{\mathbb{E}}

\title[]{Basic tail and concentration bounds}
\author[Kaveh S. Nobari]{Kaveh S. Nobari}
\institute[]{Lectures in High-Dimensional Statistics}
\date[27/10/2020]
{Department of Mathematics and Statistics\\ Lancaster University}
	

\begin{document}
\begin{frame}
\titlepage
\end{frame}


\begin{frame}{Contents}
\tableofcontents
\end{frame}

\begin{frame}[allowframebreaks]{Motivation}
It is often of interest to obtain bounds on the tails of a random variable, or two-sided inequalities, which guarantee that the random variable is close to its mean or median. These slides follow the structure of chapter 2 of \citet{wainwright2019high} to shed light on the elementary techniques for obtaining \textcolor{red}{deviation} and \textcolor{red}{concentration} inequalities.
{\vskip 0.5em}
One way of controlling a tail probability $P[X\geq t]$ is by controlling the moments of the random variable $X$, where by controlling higher-order moments of the variable $X$, we can obtain sharper bounds on tail probabilities. This motivates the "Classical bounds" section of the notes. 
{\vskip0.5em}
We then extend the derivation of bounds to more general functions of the random variables in the "Martingale-based methods" section using \textcolor{red}{martingale decompositions}, as opposed to limiting the techniques to deriving bounds on \textcolor{red}{the sum of independent} random variables.  
{\vskip  0.5em}
Finally, the seminar is concluded with a classical result on the concentration properties of Lipschitz functions of Gaussian variables. 
\end{frame}

\section{Classical bounds}
\subsection{From Markov to Chernoff}
\frame{\tableofcontents[currentsection , currentsubsection]}

%------------------------------------------------
\begin{frame}[allowframebreaks]
The most elementary tail bound is \textcolor{red}{Markov's inequality}:
\begin{block}{Markov's inequality}
Given a non-negative random variable $X$ with finite mean - i.e. $\E[X]<\infty$, we have
\[
P[X\geq t]\leq\frac{\E[X]}{t},\quad \forall t>0
\]
\end{block}
It is immediately obvious that Markov's inequality \textcolor{red}{requires only the existence of the first moment}. If the random variable $X$ also has finite variance - i.e. $\text{var}(X)<\infty$, we have \textcolor{red}{Chebyshev's inequality}:
\begin{block}{Chebyshev's inequality}
For a random variable $X$ that has a finite mean and variance, we have
\[
P[\lvert X-\mu \rvert\geq t]\leq\frac{\text{var}(X)}{t^2},\quad \forall t>0
\]
\end{block} 
\end{frame}
\begin{frame}
\begin{proof}
Chebyshev's inequality follows from Markov's inequality, by considering the variable $(X-\E[X])^2$ and the constant $t^2$. By substituting these in the Markov inequality, we get
\begin{align}
P[(X-\E[X])^2\geq t^2]&\leq \frac{\E[(X-\E[X])^2]}{t^2}\\
P[\lvert X-\E[X] \rvert\geq t]&\leq\frac{\E[(X-\E[X])^2]}{t^2}
\end{align}
Since, $\E[X]=\mu$ and $\text{var}(X)=\E[(X-\E[X])^2]$, we get
\[
P[\lvert X-\mu \rvert\geq t]\leq\frac{\text{var}(X)}{t^2}
\]
\end{proof}
\end{frame}

\begin{frame}[allowframebreaks]
The earlier results can be generalised as follows:
\begin{block}{Extensions of Markov's inequality}
Whenever a variable $X$ has a central moment of order $k$, an application of Markov's inequality to the random variable $\lvert X-\mu\rvert^k$ yields:
\[
P[\lvert X-\mu\rvert\geq t]\leq\frac{\E[\lvert X-\mu \rvert]^k}{t^k},\quad \forall t>0.
\]
\end{block}
and this is not limited to polynomials $\lvert X-\mu \rvert^k $:

Suppose $X$ has a mgf in a neighbourhood of zero,such that there is a constant $b>0$ that the functions $\rho(\lambda)=\E[\exp(\lambda(X-\mu))]$ exists for all $\lambda<\lvert b\rvert$. Thus, for any $\lambda\in[0,b]$, we may apply Markov's inequality to the random variable $Y=\exp(\lambda(X-\mu))$, obtaining the upper bound:
\[
P[(X-\mu)\geq t]=P[\exp(\lambda(X-\mu))\geq \exp(\lambda t)]\leq\frac{\E[\exp(\lambda(X-\mu))]}{\exp(\lambda t)}
\]
By taking the $\log$ of both side of the latter inequality, we get:
\[
\log P[(X-\mu)\geq t]\leq \log\E[\exp(\lambda(X-\mu))]-\lambda t
\]
Optimising, our choice of $\lambda$, we can obtain the tightest results that yields the Chernoff bound:
\begin{block}{Chernoff bound}
\[
\log P[(X-\mu)\geq t]\leq \inf_{\lambda\in [0,b]}\{\log\E[\exp(\lambda(X-\mu))]-\lambda t\}.
\]
\end{block}

\end{frame}


\subsection{Sub-Gaussian variables and Hoeffding bounds}
\frame{\tableofcontents[currentsubsection]}

\begin{frame}[allowframebreaks]
Evidently, the form of the tail bound obtained using the Chernoff approach depends on the growth rate of the mgf. Naturally, in the study of the tail bounds the random variables are then classified in terms of their mgfs. The simplest type of behaviour is known as sub-Gaussian, which shall be motivated by deriving tail bounds for a Gaussian variables, say, X, such that $X\sim N(\mu,\sigma^2)$, with density
\[
f(X)=\frac{1}{\sigma\sqrt{2\pi}}\exp\left(-\frac{(X-\mu)^2}{2\sigma^2}\right)
\]
and thus, the mgf
\[
\E[\exp(\lambda X)]=\exp\left(\mu\lambda+\frac{\sigma^2\lambda^2}{2}\right),\quad\forall\lambda\in\mathbb{R}
\]
\begin{example}[Gaussian tail bounds]
Let $X\sim N(\mu,\sigma^2)$ be a Gaussian r.v., which has mgf
\[
\E[\exp(\lambda X)]=\exp\left(\mu\lambda+\frac{\sigma^2\lambda^2}{2}\right),\quad \forall \lambda\in \mathbb{R}
\]
substituting this into the optimising problem of the Chernoff bound, we get
\begin{equation}\label{eq: optms}
\inf_{\lambda\geq 0}\{\log\E[\exp(\lambda(X-\mu))-\lambda t]\}=\inf_{\lambda\geq 0}\left\{\frac{\sigma^2\lambda^2}{2}-\lambda t\right\}=-\frac{t^2}{2\sigma^2}
\end{equation}
Therefore, we can conclude that any $N(\mu,\sigma^2)$ r.v. satisfies the upper deviation inequality
\begin{equation}\label{eq: Sub-Gaussian TB}
P[X\geq\mu+t]\leq\exp\left(-\frac{t^2}{2\sigma^2}\right)
\end{equation}
\end{example}
\end{frame}
\begin{frame}
\begin{proof}[Proof of equation (\ref{eq: optms})]
To solve the optimisation problem below 
\[
\inf_{\lambda\geq 0}\left\{\frac{\sigma^2\lambda^2}{2}-\lambda t\right\}
\]
we take derivatives to find the optimum of this quadratic function, - i.e.
\[
\frac{\partial}{\partial\lambda}\left(\frac{\sigma^2\lambda^2}{2}-\lambda t\right)=0,
\]
which leads to $\lambda_{opt}=\frac{t}{\sigma^2}$. Substituting $\lambda_{opt}$ with $\lambda$ in the above equation yields relationship (\ref{eq: optms}).
\end{proof}
\end{frame}

\begin{frame}
\begin{definition}[Sub-Gaussianity]
A r.v. $X$ with mean $\mu=\E[X]$ is sub-Gaussian if there is a positive number $\sigma$, such that
\[
\E[\exp(\lambda(X-\mu))]\leq\exp\left(\frac{\sigma^2\lambda^2}{2}\right),\quad\forall\lambda\in\mathbb{R}
\]
where the constant $\sigma$ is referred to as the \textcolor{red}{sub-Gaussian parameter}. Moreover, by the symmetry of the definition, the variable $-X$ is sub-Gaussian iff $X$ is sub-Gaussian, so that we also have lower deviation inequality $P[X\leq\mu-t ]\leq\exp\left(-\frac{t^2}{2\sigma^2}\right), \quad\forall t\geq 0$. Thus, we conclude that any sub-Gaussian variable satisfies the \textcolor{red}{concentration inequality}
\[
P[\lvert X-\mu\rvert\geq t]\leq2\exp\left(-\frac{t^2}{2\sigma^2}\right),\quad\forall t\in\mathbb{R}.
\]
\end{definition}
\end{frame}
\begin{frame}
We may have scenarios in which sub-Gaussian variables are non-Gaussian.
\begin{example}[Rademacher variables]
A Rademacher r.v. $\varepsilon$ takes the values $[-1,+1]$ equiprobably -i.e $P[\varepsilon=-1]=P[\varepsilon=+1]=\frac{1}{2}$. Thus, the mgf of $\varepsilon$ is as follows
\[
\E[\exp(\lambda \varepsilon)]=\sum_{i\in\{-1,+1\}} \exp(\lambda\varepsilon_i)p(\varepsilon=i)=\frac{1}{2}[\exp(-\lambda)+\exp(\lambda)]
\]
where the Maclaurin-series expansion of the terms $\exp(-\lambda)$ and $\exp(\lambda)$ leads gives us
\begin{align*}
\E[\exp(\lambda \varepsilon)]=\frac{1}{2}\left[\sum\limits_{k=0}^{\infty}\frac{\lambda^k}{k!}+\sum\limits_{k=0}^{\infty}\frac{(-\lambda)^k}{k!}\right]&=\frac{1}{2}\left[2\sum\limits_{k=0}^{\infty}\frac{\lambda^{2k}}{2k!}\right]=\sum\limits_{k=0}^{\infty}\frac{\lambda^{2k}}{2k!}\\
&\leq 1+\sum\limits_{k=0}^{\infty}\frac{\lambda^{2k}}{2^kk!}=\exp\left(\frac{\lambda^2}{2}\right)
\end{align*}
with the sub-Gaussian parameter $\sigma=1$.
\end{example}
\end{frame}

\begin{frame}
\textbf{Some preliminaries:}  The exponential function $g(z)=\exp(z)$ is convex; thus, Jensen's inequality for convex functions applies as follows
\[
g(\E[z])\leq \E[g(z)]
\] 

A r.v. $Z'$ is an \textcolor{red}{independent copy} of $Z$, if it has a same the same distribution as $Z$, and where $Z$ and $Z'$ are independent. 

Given the above definitions, we provide a simple example of \textcolor{red}{symmetrization argument}, in which first an independent copy of $X$, $X'$ is introduced and the problem is symmetrized using a Rademacher variable.
\end{frame}

\begin{frame}
\begin{block}{Symmetrization argument}
Let $X$ be a r.v. with mean zero - i.e. $\mu=\E_X[X]=0$, with a support on the interval $[a,b]$, and let $X'$ be an independent copy of $X$, for any $\lambda\in\mathbb{R}$, we have
\[
\E_X[\exp(\lambda X)]=\E_X[\exp(\lambda(X-\E_{X'}[X']))]
\]
since $\E_X[X]=\E_{X'}[X']=0$. Using Jensen's inequality, we further establish that
\[
\E_X[\exp(\lambda(X-\E[X']))]\leq \E_{X,X'}[\exp(\lambda(X-X'))] 
\]
Further, note that $\varepsilon(X-X')$ and $(X-X')$ possess the same distribution, where $\varepsilon$ is a Rademacher r.v., so that
\[
\E_{X,X'}[\exp(\lambda(X-X'))]=\E_{X,X'}[\E_{\varepsilon}[\exp(\lambda\varepsilon(X-X'))]]¸
\]
\end{block}
\end{frame}
\begin{frame}¸
\begin{block}{}
where from the earlier example, we know that
\[
\E_{X,X'}[\E_{\varepsilon}[\exp(\lambda\varepsilon(X-X'))]]\leq \E_{X,X'}\left[\exp\left(\frac{\lambda^2(X-X')^2}{2}\right)\right]
\]
since $\lvert X-X' \rvert\leq b-a$, we are guaranteed that
\[
\E_{X,X'}\left[\exp\left(\frac{\lambda^2(X-X')^2}{2}\right)\right]\leq\exp\left(\frac{\lambda^2(b-a)^2}{2}\right)
\]
thus, we have shown that $X$ is sub-Gaussian with sub-Gaussian parameter $\sigma=b-a$
\end{block}
\end{frame}
\begin{frame}

\textbf{Quiz:}
{\vskip 0.5em}
\begin{itemize}
\item[1)]Two independent sub-Gaussian variables $X_1$ and $X_2$ possess the sub-Gaussian parameters $\sigma_1$ and $\sigma_2$ respectively. What is the sub-Gaussian parameter of $X_1+X_2$?

\item[2)] Now once again consider the sub-Gaussian tail bound (\ref{eq: Sub-Gaussian TB}). How is this result extended to the variable $X_1+X_2$?
\end{itemize}
\end{frame}

\begin{frame}
The answers to the above quiz, can be generalised to the variables $X_1,\cdots,X_n$ with mean $\mu_i$ and sub-Gaussian parameters $\sigma_i$ for $i=1,\cdots,n$ leading to the Hoeffding bound 
\begin{block}{Hoeffding bounds}
Suppose that the variables $X_1,\cdots,X_n$ each with mean $\mu_1,\cdots,\mu_n$ and sub-Gaussian parameter $\sigma_1,\cdots,\sigma_n$ are independent. Then we have
\[
P\left[\sum\limits_{i=1}^n(X_i-\mu_i)\geq t\right]\leq\exp\left\{-\frac{t^2}{2\sum_{i=1}^n\sigma_i^2}\right\}
\]
\end{block}
\end{frame}
\begin{frame}[allowframebreaks]
To prove the equivalent characterizations of sub-Gaussian variables, it is of interest to first answer Exercise 2.2 of \citet{wainwright2019high} which introduces \textcolor{red}{Mills ratio}.

\textbf{Exercise 2.2 of \citet{wainwright2019high}:} Let $\phi(z)=\frac{1}{\sqrt{2\pi}}\exp\left(\frac{-z^2}{2}\right)$ be the density function of a standard normal $Z\sim N(0,1)$ variate.
\begin{itemize}
\item[1)]Show that $\phi'(z)+z\phi(z)=0$
\item[2)] Use part 1 to show that
\[
\phi(z)\left(\frac{1}{z}-\frac{1}{z^3}\right)\leq P[Z\geq z]\leq\phi(z)\left(\frac{1}{z}-\frac{1}{z^3}+\frac{3}{z^5}\right),\quad \forall z>0
\]
\end{itemize}
\textbf{Solution:}

\underline{Part 1:}
\[
\phi'(z)=-\frac{z}{\sqrt{2\pi}}\exp\left(\frac{-z^2}{2}\right)\quad \text{and}\quad z\phi(z)=\frac{z}{\sqrt{2\pi}}\exp\left(\frac{-z^2}{2}\right) 
\]
thus,
\[
\phi'(z)+z\phi(z)=0
\]
\underline{Part 2:} 

Note that $P[Z\geq z]=\int_{z}^{\infty}\phi(t)dt$. Furthermore, from part 1, we know that $\phi(z)=\frac{-\phi'(z)}{z}$. By substituting $\frac{-\phi'(z)}{z}$ into the earlier integral, we get
\[
\int_{z}^{\infty}\phi(t)dt=\int_{z}^{\infty}\frac{-\phi'(t)}{t}dt=\left[\frac{-\phi'(t)}{t}\right]_{z}^{\infty}-\int_{z}^{\infty} \frac{\phi(t)}{t^2}dt
\]
We know that $\lim_{t\to \infty}\frac{-\phi'(t)}{t}=0$, therefore, we may apply the the above expression can be written as
\[
\frac{\phi(z)}{z}-\int_{z}^{\infty} \frac{-\phi'(t)}{t^3}dt
\] 
using the substitution derived from Miller's ratio. Using integration by parts yet again, we obtain
\begin{align*}
\frac{\phi(z)}{z}-\int_{z}^{\infty} \frac{-\phi'(t)}{t^3}dt&=\frac{\phi(z)}{z}+\left[\frac{\phi(t)}{t^3}\right]_{z}^{\infty}-\int_{z}^{\infty}\frac{-3\phi(t)}{t^4}dt\\
&=\frac{\phi(z)}{z}+\frac{\phi(z)}{z^3}+\int_{z}^{\infty}\frac{3\phi(t)}{t^4}dt\\
P[Z\geq z]&=\phi(z)\left(\frac{1}{z}+\frac{1}{z^3}\right)+\underbrace{\int_{z}^{\infty}\frac{3\phi(t)}{t^4}dt}_{\geq0}\\
		&\geq \phi(z)\left(\frac{1}{z}+\frac{1}{z^3}\right)
\end{align*}
Applying the same procedure again will prove the upper inequality. This is left as an exercise to the reader.
\end{frame}
\begin{frame}
\begin{block}{Equivalent characterizations of the sub-Gaussian variables (I-II)}
\begin{itemize}
\item[(I)] From the definition of sub-Gaussian variables, a r.v. with $\mu=\E[X]=0$ is sub-Gaussian for $\sigma\geq 0$,
\[
\E[\exp(\lambda X)]\leq\exp\left(\frac{\sigma^2\lambda^2}{2}\right),\quad\forall\lambda\in\mathbb{R}
\]
\item[(II)] There is a constant $c\geq 0$ and Gaussian r.v. $Z\sim N(0,\tau^2)$, such that
\[
P[\lvert X\rvert\geq s]\leq cP[\lvert Z\rvert\geq s],\quad\forall s\geq0.
\]
\end{itemize}
\end{block}
\end{frame}
\subsection{Sub-exponential variables and Bernstein bounds}
\frame{\tableofcontents[currentsection,currentsubsection]}
\begin{frame}
The notion of sub-Gaussianity is rather restrictive. We thus now introduce sub-exponential variables, which impose milder conditions on the mgf.
\begin{definition}{Sub-exponentiality}
A r.v. $X$ with mean $\mu=\E[X]$ is sub-exponential if there are non-negative parameters $(\nu,\alpha)$, such that
\[
\E[\exp(\lambda(X-\mu))]\leq\exp\left(\frac{\nu^2\lambda^2}{2}\right),\quad\forall \lvert\lambda\rvert<\frac{1}{\alpha}
\]
It is immediately obvious that any sub-Gaussian variable is also sub-exponential, where the former is a special case of the latter, with $\nu=\sigma$ and $\alpha=0$. However, the converse is not true. 
\end{definition}
An example of a case where a variable is sub-exponential but not sub-Gaussian is as follows
\end{frame}

\begin{frame}
\begin{example}[sub-exponential but not sub-Gaussian]
Let $Z\sim N(0,1)$, and consider the r.v. $X=Z^2$, such that $Z\sim \chi^2_1$. Therefore, the mean $\mu=\E[\chi_1^2]=1$. For $\lambda<\frac{1}{2}$, we have the mgf as follows
\begin{align*}
\E[\exp(\lambda(X-1))]&=\int_{-\infty}^{+\infty}\exp(\lambda(Z^2-1))f(z)dz\\
&=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^{+\infty}\exp(\lambda(Z^2-1))\exp\left(\frac{-Z^2}{2}\right)dz\\
&=\frac{\exp(-\lambda)}{\sqrt{1-2\lambda}}.
\end{align*}
for $\lambda\geq\frac{1}{2}$ the mgf is infinite, which reveals that $X$ is not sub-Gaussian.
\end{example}
\end{frame}

\subsection{Some one-sided results}
\frame{\tableofcontents[currentsection,currentsubsection]}

\section{Martingale-based methods}
\frame{\tableofcontents[currentsection]}

\section{Lipschitz functions of Gaussian variables}
\frame{\tableofcontents[currentsection]}





\begin{frame}[allowframebreaks]
\frametitle{References}
\bibliographystyle{apa}
\bibliography{References_HDStat}
\end{frame}

\end{document}