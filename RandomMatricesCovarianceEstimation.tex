\documentclass[10pt,handout,english]{beamer}
\usetheme{Warsaw}
\usepackage{graphicx}
\usepackage{placeins}
\usepackage{amsmath, amssymb}
\usepackage{tabu}
\usepackage{bbm}
\usepackage{booktabs}
\usepackage[round]{natbib}
\usepackage{bm}
\usepackage{ragged2e}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{xcolor}
\usepackage[super]{nth}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=blue,      
    urlcolor=blue,
    citecolor=black,
}

\apptocmd{\frame}{}{\justifying}{} % Allow optional arguments after frame.

\setbeamertemplate{frametitle continuation}{}

\newcommand\setItemnumber[1]{\setcounter{enumi}{\numexpr#1-1\relax}}

\DeclareMathOperator{\tr}{tr}

\newcommand{\ts}{\textsuperscript}
\newcommand{\E}{\mathbb{E}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\F}{\mathcal{F}}
\newcommand{\vertiii}[1]{{\left\vert\kern-0.25ex\left\vert\kern-0.25ex\left\vert #1 
    \right\vert\kern-0.25ex\right\vert\kern-0.25ex\right\vert}}


\title[]{Random matrices and covariance estimation}
\author[Kaveh S. Nobari]{Kaveh S. Nobari}
\institute[]{Lectures in High-Dimensional Statistics}
\date[27/10/2020]
{Department of Mathematics and Statistics\\ Lancaster University}
	

\begin{document}
\begin{frame}
\titlepage
\end{frame}


\begin{frame}{Contents}
\tableofcontents
\end{frame}

\begin{frame}[allowframebreaks]{Motivation}
The issue of covariance estimation is intertwined with random matrix theory, since sample covariance is a particular type of random matrix. These slides follow the structure of chapter 6 of \citet{wainwright2019high} to shed light on random matrices in a \textcolor{red}{non-asymptotic setting}, with the aim of \textcolor{red}{obtaining explicit deviation inequalities that hold for all sample sizes and matrix dimensions.}
{\vskip 0.5em}
In the classical framework of covariance matrix estimation the sample size $n$ tends to infinity while the matrix dimension $d$ is fixed; in this setting the behaviour of sample covariance matrix is characterized by the usual limit theory. In contrast, in high-dimensional settings the data dimension is either comparable to the sample size $(d\asymp n)$ or possibly much larger than the sample size $d\gg n$.
{\vskip0.5em}
We begin with the simplest case, namely ensembles of Gaussian random matrices, and we then discuss more general sub-Gaussian ensembles, before moving to milder tail conditions.  
\end{frame}

\section{Preliminaries}
\subsection{Notations in linear algebra}
\frame{\tableofcontents[currentsection]}



%------------------------------------------------
\begin{frame}[allowframebreaks]
First, let us consider \textcolor{red}{rectangular matrices}, for instance matrix $A\in\R^{n\times m}$ with $n\geq m$, the ordered singular values are written as follows
\[
\sigma_{\max}(A)=\sigma_1(A)\geq\sigma_{2}(A)\geq\cdots\geq\sigma_m(A)=\sigma_{\min}(A)\geq 0
\]
The maximum and minimum singular values are obtained by maximizing the \textquotedblleft blow-up factor\textquotedblright
\[
\sigma_{\max}(A)=\max_{\forall x}\frac{\lVert Ax \rVert_2}{\lVert x\rVert_2},\quad \sigma_{\min}(A)=\min_{\forall x}\frac{\lVert Ax \rVert_2}{\lVert x\rVert_2}
\]
which is obtained when $x$ is the largest and smallest singular vectors respectively - i.e.
\[
\sigma_{\max}(A)=\max_{v\in S^{m-1}}\frac{\lVert Av \rVert_2}{\lVert v\rVert_2},\quad \sigma_{\min}(A)=\min_{v\in S^{m-1}}\frac{\lVert Av \rVert_2}{\lVert v\rVert_2}
\]
noting that $\lVert v\rVert_2=1$, since $S^{d-1}:=\{v\in\R^d\mid \lVert v\rVert_2=1\}$ is the Euclidean unit sphere in $\R^d$. We may denote
\[
\vertiii{A}_2=\sigma_{\max}(A)
\]
However, \textcolor{red}{covariance matrices are square symmetric matrices}, thus we must also focus on symmetric matrices in $\R^d$, denoted $S^{d\times d}:=\{Q\in\R^{d\times d}\mid Q=Q'\}$, as well as subset of semi-definite matrices given by
\[
S_{+}^{d\times d}:=\{Q\in S^{d\times d}\mid Q\geq 0\}.
\]
Any matrix $Q\in S^{d\times d}$ is diagonalizable via unitary transformation, and let us denote the vector of eigenvalues of $Q$ by $\gamma(Q)\in\R^d$ ordered as 
\[
\gamma_{\max}(Q)=\gamma_1(Q)\geq \gamma_2(Q) \geq\cdots\geq\gamma_d(Q)=\gamma_{\min}(Q)
\]
Note the matrix $Q$ is semi-positive definite, which may be expressed as $Q\geq 0$, iff $\gamma_{\min}(Q)\geq 0$.

The Rayleigh-Ritz variational characterization of the minimum and maximum eigenvalues 
\[
\gamma_{\max}(Q)=\max_{v\in S^{d-1}}v'Qv\quad\text{and}\quad\gamma_{\min}(Q)=\min_{v\in S^{d-1}}v'Qv
\]
For symmetric matrix $Q$, the $l_2$ norm can be expressed as 
\[
\vertiii{Q}_2=\max\{\gamma_{\max}(Q), \lvert\gamma_{\min}(Q)\rvert\}:=\max_{v\in S^{d-1}}\lvert v'Qv\rvert
\]
Finally, suppose we have a rectangular matrix $A\in\R^{n\times m}$, with $n\geq m$. We know that any rectangular matrix can be expressed using singular value decomposition (SVD hereafter), as follows 
\[
A=U\Sigma V'
\]
wher $U$ is an $n\times n$ unitary matrix, $\Sigma$ is an $n\times m$ rectangular diagonal matrix with non-negative real numbers on the diagonal up and $V$ is an $n\times n$ unitary matrix. Using SVD, we can express $A'A$ where
\[
A'A=V\Sigma' U'U\Sigma V'
\]
and since $U$ is an orthogonal matrix, we know that $U'U=I$ where $I$ is the identity matrix.
\[
A'A=V(\Sigma'\Sigma) V'
\]
Therefore, as the diagonal matrix $\Sigma$ contains the eigenvalues of matrix $A$, hence, $\Sigma'\Sigma$ contains the eigenvalues of $A'A$ and it can be thus concluded
\[
\gamma_j(A'A)=(\sigma_j(A))^2,\quad j=1,\cdots,m
\]
\end{frame}

\subsection{Set-up of covariance estimation}
\frame{\tableofcontents[currentsection]}

\begin{frame}[allowframebreaks]
Let $\{x_1,\cdots,x_n\}$ be a collection of $n$ i.i.d samples from a distribution in $\R^d$ with zero mean and the covariance matrix $\Sigma$. A standard estimator of sample covariance matrix is
\[
\hat{\Sigma}:=\frac{1}{n}\sum\limits_{i=1}^{n}x_ix_i'.
\]
Since, each $x_i$ for $i=1,\cdots,n$ has zero mean, it is guaranteed that
\[
\E[x_ix_i']=\Sigma
\]
and the random matrix $\hat{\Sigma}$ is an \textcolor{red}{unbiased} estimator of the population covariance $\Sigma$. Consequently the error matrix $\hat{\Sigma}-\Sigma$ has mean zero, and \textcolor{red}{goal is to obtain bounds on the error measures in $l_2$-norm}. We are essentially seeking a band of the form
\[
\vertiii{\hat{\Sigma}-\Sigma}_2\leq\varepsilon,
\]
where,
\begin{align*}
\vertiii{\hat{\Sigma}-\Sigma}_2&=\max_{v\in S^{d-1}}\left\lvert v'\left\{\frac{1}{n}\sum\limits_{i=1}^{n}x_ix_i'-\Sigma\right\}v \right\rvert\\
&=\max_{v\in S^{d-1}}\left\lvert\frac{1}{n}\sum\limits_{i=1}^{n}v'x_ix_i'v-v'\Sigma v \right\rvert\\
&=\max_{v\in S^{d-1}}\left\lvert\frac{1}{n}\sum\limits_{i=1}^{n}\langle x_i,v_i\rangle^2-v'\Sigma v \right\rvert\leq\varepsilon
\end{align*}
which suggests that controlling the deviation $\vertiii{\hat{\Sigma}-\Sigma}_2$ is equivalent to establishing a ULLN for the class of functions $x\to\langle x,v \rangle^2$, indexed by vectors $v\in S^{d-1}$.

\begin{definition}[Weyl's Inequality]
\begin{itemize}
\item[(I)]Given any \textcolor{red}{real symmetric matrices} A, B,
\begin{align*}
\gamma_1(A+B)\geq \gamma_1(A)+\gamma_1(B)\\
\gamma_n(A+B)\leq\gamma_n(A)+\gamma_n(B)
\end{align*}
\item[(II)]Given any \textcolor{red}{real symmetric matrices} A, B,
\[
\lvert \gamma_k(A)-\gamma_k(B)\rvert\leq\vertiii{(A-B)}_2
\]
(see \citet{dasgupta2008asymptotic}).
\end{itemize}Â¸
\end{definition}
Control in the operator norm further guarantees that the eigenvalues of $\hat{\Sigma}$ are uniformly close to those of $\Sigma$. Furthermore, given Weyl's inequality II above, we have
\[
\max_{j=1,\cdots,d}\lvert \gamma_j(\hat{\Sigma})-\gamma_j(\Sigma)\rvert\leq\vertiii{\hat{\Sigma}-\Sigma}_2
\]
Note that the random matrix $X\in\R^{n\times d}$ has the vectors $x_i'$ on its $i\ts{th}$ row and singular values denotes by $\{\sigma_j(X)\}_{j=1}^{\min{n,d}}$. Thus,
\[
\hat{\Sigma}=\frac{1}{n}\sum\limits_{i=1}^n x_ix_i'=\frac{1}{n}X'X
\]
and hence, the eigenvalues of $\hat{\Sigma}$ are the squares of the singular values of $X/\sqrt{n}$.
\end{frame}
\section{Wishart matrices and their behaviour}
\frame{\tableofcontents[currentsection]}

\begin{frame}
\begin{definition}[Gaussian ensembles and Wishart distribution]
Suppose that each sample $x_i$ of a matrix $X\in\R^{n\times d}$ is drawn from an i.i.d multivariate $N(0,\Sigma)$ distribution. In this case we say that the associated matrix $X\in n\times d$, with $x_i'$ and its $i\ts{th}$ row, is drawn from the $\Sigma$-Gaussian ensemble. The associated sample covariance $\hat{\Sigma}=\frac{1}{n}X'X$ is said to follow a multivariate Wishart distribution.
\end{definition}
Following \citet{wainwright2019high}, we present deviation inequalities for $\Sigma$-Gaussian ensembles and present a few examples before proving said inequalities.
\end{frame}
\begin{frame}
\begin{theorem}
Let $X\in\R^{n\times d}$ be drawn according to the $\Sigma$-Gaussian ensemble. Then for $\delta>0$, the maximum singular value $\sigma_{\max}(X)$ satisfies the upper deviation inequality
\[
P\left[\frac{\sigma_{\max}(X)}{\sqrt{n}}\geq \gamma_{\max}(\sqrt{\Sigma})(1+\delta)+\sqrt{\frac{\tr(\Sigma)}{n}}\right]\leq\exp\left(-\frac{n\delta^2}{2}\right)
\]
Furthermore, for $n\geq d$, the minimum singular value $\sigma_{\min}(X)$ satisfies the lower deviation inequality
\[
P\left[\frac{\sigma_{\min}(X)}{\sqrt{n}}\leq \gamma_{\min}(\sqrt{\Sigma})(1-\delta)-\sqrt{\frac{\tr(\Sigma)}{n}}\right]\leq\exp\left(-\frac{n\delta^2}{2}\right)
\]
\end{theorem} 
\end{frame}
\begin{frame}[allowframebreaks]
\textbf{Example (Norm bounds for standard Gaussian ensemble):} 
Consider $W\in\R^{n\times d}$ generated with i.i.d $N(0,1)$ entries, which leads to the $I_d$-Gaussian ensemble. Given the above Theorem, it can be concluded that for $n\geq d$
\[
\frac{\sigma_{\max}(W)}{\sqrt{n}}\leq 1+\delta+\sqrt{\frac{d}{n}}\quad\text{and}\quad \frac{\sigma_{\min}(W)}{\sqrt{n}}\geq 1-\delta-\sqrt{\frac{d}{n}}
\]
Now it is evident that
\[
1-P\left[\frac{\sigma_{\max}(W)}{\sqrt{n}}\geq 1+\delta+\sqrt{\frac{d}{n}}\right]=
P\left[\frac{\sigma_{\max}(W)}{\sqrt{n}}\leq 1+\delta+\sqrt{\frac{d}{n}}\right]
\]
thus according to the earlier Theorem,
 \[
P\left[\frac{\sigma_{\max}(W)}{\sqrt{n}}\leq 1+\delta+\sqrt{\frac{d}{n}}\right]\geq1- \exp{\left(-\frac{n\delta^2}{2}\right)}
\]
and similarly
 \[
P\left[\frac{\sigma_{\min}(W)}{\sqrt{n}}\geq 1-\delta-\sqrt{\frac{d}{n}}\right]\geq 1-\exp{\left(-\frac{n\delta^2}{2}\right)}
\]
Thus, it can easily be seen that both bounds hold with probability greater than $1-2\exp\left(-\frac{n\delta^2}{2}\right)$. As we recall, the eigenvalues of the symmetric covariance matrix $\hat{\Sigma}$ is the square of the singular values $W/\sqrt{n}$. Furthermore,
\begin{align*}
\vertiii{\hat{\Sigma}-\Sigma}_2&=\max_{v\in S^{d-1}}\left\lvert v'\left\{\frac{1}{n}W'W-I_d\right\}v\right\rvert\\
&=\max_{v\in S^{d-1}}\left\lvert \frac{1}{n}v'(W'W)v-v'I_dv\right\rvert
\end{align*}
Note that $v'I_dv=\lVert v\rVert_2^2=1$. Thus,
\begin{align*}
\vertiii{\hat{\Sigma}-\Sigma}_2&=\vertiii{\frac{1}{n}W'W-I_d}_2\\
&=\max_{v\in S^{d-1}}\left\lvert \frac{1}{n}v'(W'W)v-1\right\rvert
\end{align*}
Moreover, we have
\[
\frac{\sigma_{\max}(W)}{\sqrt{n}}\leq 1+\delta+\sqrt{\frac{d}{n}}
\]
or
\begin{align*}
\frac{(\sigma_{\max}(W))^2}{n}&\leq1+2\left(\underbrace{\delta+\sqrt{\frac{d}{n}}}_{\varepsilon}\right)+ \left(\underbrace{\delta+\sqrt{\frac{d}{n}}}_{\varepsilon}\right)^2\\
\left\{\frac{(\sigma_{\max}(W))^2}{n}-1\right\}&\leq2\varepsilon+ \varepsilon^2
\end{align*}
thus,
\[
\vertiii{\frac{1}{n}W'W-I_d}_2\leq 2\varepsilon+\varepsilon^2
\]
Note that $\frac{d}{n}\to 0$, thus, the sample covariance matrix $\hat{\Sigma}$ is a consistent estimate of the identity matrix $I_d$.

\textbf{Example (Gaussian covariance estimation):} 

Let $X\in\R^{n\times d}$ be a random matrix from the $\Sigma$-Gaussian ensemble. Noting that a if $X\sim N(0,\Sigma)$ it can equivalently be written as $X\sim\sqrt{\Sigma} N(0,I_d)$. So assuming that $W\sim N(0,I_d)$, we may express $X$ as $X=W\sqrt{\Sigma}$. Moreover, 
\begin{align*}
\vertiii{\frac{1}{n}X'X-\Sigma}_2&=\vertiii{\sqrt{\Sigma}\left(\frac{1}{n}W'W-I_d\right)\sqrt{\Sigma}}_2\\
&\leq\vertiii{\Sigma}_2\vertiii{\frac{1}{n}W'W-I_d}_2
\end{align*}
Thus, given the earlier example we know that
\[
\vertiii{\frac{1}{n}W'W-I_d}_2\leq2\varepsilon+\varepsilon^2,
\]
where $\varepsilon=\delta+\sqrt{\frac{d}{n}}$. Therefore, 
\[
\frac{\vertiii{\hat{\Sigma}-\Sigma}_2}{\vertiii{\Sigma}_2}\leq 2\varepsilon+\varepsilon^2
\]
Therefore, the relative error above converges to zero, so long as $d/n\to 0$.
\end{frame}
\begin{frame}[allowframebreaks]
To show the proof for the earlier Theorem first we recap a concept from the concentration inequalities chapter:
\begin{block}{Recap (Theorem 2.26 of Wainwright):}
Let $(X_1,\cdots,X_n)$ be a vector of i.i.d standard Gaussian variables, and let $f:\R^n\to\R$ be $L$-Lipschitz wrt to the Euclidean norm. Then the variable $f(X)-\E[f(X)]$ is sub-Gaussian with parameter at most L, and hence
 \[
P[\lvert f(X)-E[f(X)]\rvert\geq t]\leq2\exp\left(-\frac{t^2}{2L^2}\right),\quad \forall t\geq 0
\]
\end{block}

\textbf{Example (Singular values of Gaussian random matrices):} For $n > d$, let $X\in \R^{n\times d}$ be a random matrix with i.i.d. $N(0,1)$ entries, and let
\[
\sigma_1(X)\geq \sigma_2(X)\geq\cdots\geq\sigma_d(X)\geq 0 
\]
are the ordered singular values of the matrix $X$. Referring to Weyl's inequality II, and given another matrix $Y\in\R^{n\times d}$, we have
\[
\max_{k=1,\cdots,d}\lvert \sigma_k(X)-\sigma_k(Y)\rvert \leq \vertiii{X-Y}_2\leq\vertiii{X-Y}_F
\]
where $\vertiii{.}_F$ denotes the Frobenius norm. Recalling that an $L$-Lipschitz function is one for which
\[
\lvert f(X)-f(Y)\rvert\leq L\vertiii{X-Y}_2
\]
it can be suggested that $\sigma_k(X)$ for each $k$ is a $1$-Lipschitz function of random matrix.  Furthermore, from Theorem 2.26 of Wainwright it can be shown that
\[
P[\lvert\sigma_k(X)-\E[\sigma_k(X)]\rvert\geq \delta ]\leq2\exp\left(-\frac{\delta^2}{2}\right),\quad\forall \delta\geq 0
\]
Now we wish to show that for $X\in\R^{n\times d}$ that is drawn according to the $\Sigma$-Gaussian ensemble, the maximum singular value $\sigma_{\max}(X)$ satisfies the upper deviation inequality
 \[
P\left[\frac{\sigma_{\max}(X)}{\sqrt{n}}\geq\gamma_{\max}(\sqrt{\Sigma})(1+\delta)+\sqrt{\frac{\tr(\Sigma)}{n}}\right]\leq \exp\left(-\frac{n\delta^2}{2}\right)
\]
Let us denote $\bar{\sigma}_{\max}=\gamma_{\max}(\sqrt{\Sigma})$ and recall that we can write $X=W\sqrt{\Sigma}$, where $W\in\R^{n\times d}$ has i.i.d. $N(0,1)$ entries. 

Let us view the mapping $W\to\frac{\sigma_{\max}(W\sqrt{\Sigma})}{\sqrt{n}}$ as a real-valued function on $\R^{nd}$. Noting that
\begin{align*}
\frac{\sigma_{\max}(W\sqrt{\Sigma})}{\sqrt{n}}&:= \frac{\vertiii{W\sqrt{\Sigma} }_2}{\sqrt{n}}\\
&\leq\frac{\vertiii{W}_2\vertiii{\sqrt{\Sigma}}_2}{\sqrt{n}}
\end{align*}
Thus, it is evident that this function is Lipschitz function wrt to the Euclidean norm with constant at most $L=\bar{\sigma}_{\max}/\sqrt{n}$. Hence, by concentration of measure for Lipschitz functions of Gaussian random vectors, we conclude that
\begin{align*}
P\left[\frac{\sigma_{\max}(X)}{\sqrt{n}}-\frac{\E[\sigma_{\max}(X)]}{\sqrt{n}} \geq \delta\right]&\leq\exp\left(\frac{-\delta^2}{2L^2}\right)
\end{align*}
Substituting $\bar{\sigma}_{\max}(X)/\sqrt{n}$ for $L$ and multiplying both sides of the inequality in the probability by $\sqrt{n}$, we obtain
\begin{align*}
P[\sigma_{\max}(X)-\E[\sigma_{\max}(X)] \geq \sqrt{n}\delta]&\leq\exp\left(\frac{-n\delta^2 }{2(\bar{\sigma}_{\max})^2}\right)\\
P[\sigma_{\max}(X)\geq\E[\sigma_{\max}(X)] +\bar{\sigma}_{\max} \sqrt{n}\delta]&\leq\exp\left(\frac{-n\delta^2 }{2}\right)
\end{align*}
Therefore, it is sufficient to show that 
\[
\E[\sigma_{\max}(X)]\leq\sqrt{n}\bar{\sigma}_{\max}+\sqrt{\tr(\Sigma)}
\]
Recall that the maximum singular value has the variational representation
\[
\sigma_{\max}(X)=\max_{v'\in S^{d-1}}\lVert Xv'\rVert_2,
\]
where $S^{d-1}$ denotes the Euclidean unit sphere in $\R^d$. Since $X=W\sqrt{\Sigma}$, we may write the above expression as follows
\begin{align*}
\sigma_{\max}(X)&=\max_{v'\in S^{d-1}}\lVert W\underbrace{\sqrt{\Sigma}v'}_{v}\rVert_2\\
&=\max_{v\in S^{d-1}(\Sigma^{-1})}\lVert Wv\rVert_2\\
&=\max_{u\in S^{n-1}}\max_{v\in S^{d-1}(\Sigma^{-1})}u'Wv
\end{align*}
where $S^{d-1}(\Sigma^{-1}):=\{v\in\R^d\mid\lVert\Sigma^{-\frac{1}{2}}v\rVert\}_2=1\}$ is an ellipse. 
\end{frame}
\section{Covariance matrices from sub-Gaussian ensembles}
\frame{\tableofcontents[currentsection]}

\section{Bounds for general matrices}
\subsection{Background on matrix analysis}
\frame{\tableofcontents[currentsection,currentsubsection]}


\subsection{Tail conditions for matrices}
\frame{\tableofcontents[currentsection,currentsubsection]}


\subsection{Matrix Chernoff approach and independent decompositions}
\frame{\tableofcontents[currentsection,currentsubsection]}


\subsection{Upper tail bounds for random matrices}
\frame{\tableofcontents[currentsection,currentsubsection]}


\subsection{Consequences for covariance matrices}
\frame{\tableofcontents[currentsection,currentsubsection]}

\section{Bounds for structured covariance matrices}
\subsection{Unknown sparsity and thresholding}
\frame{\tableofcontents[currentsection,currentsubsection]}

\begin{frame}[allowframebreaks]
\frametitle{References}
\bibliographystyle{apa}
\bibliography{References_HDStat}
\end{frame}

\end{document}