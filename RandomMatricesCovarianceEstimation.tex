\documentclass[10pt,handout,english]{beamer}
\usetheme{Warsaw}
\usepackage{graphicx}
\usepackage{placeins}
\usepackage{amsmath, amssymb}
\usepackage{tabu}
\usepackage{bbm}
\usepackage{booktabs}
\usepackage[round]{natbib}
\usepackage{bm}
\usepackage{ragged2e}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{xcolor}
\usepackage[super]{nth}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=blue,      
    urlcolor=blue,
    citecolor=black,
}

\apptocmd{\frame}{}{\justifying}{} % Allow optional arguments after frame.

\setbeamertemplate{frametitle continuation}{}

\newcommand\setItemnumber[1]{\setcounter{enumi}{\numexpr#1-1\relax}}

\newcommand{\ts}{\textsuperscript}
\newcommand{\E}{\mathbb{E}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\F}{\mathcal{F}}
\newcommand{\vertiii}[1]{{\left\vert\kern-0.25ex\left\vert\kern-0.25ex\left\vert #1 
    \right\vert\kern-0.25ex\right\vert\kern-0.25ex\right\vert}}


\title[]{Random matrices and covariance estimation}
\author[Kaveh S. Nobari]{Kaveh S. Nobari}
\institute[]{Lectures in High-Dimensional Statistics}
\date[27/10/2020]
{Department of Mathematics and Statistics\\ Lancaster University}
	

\begin{document}
\begin{frame}
\titlepage
\end{frame}


\begin{frame}{Contents}
\tableofcontents
\end{frame}

\begin{frame}[allowframebreaks]{Motivation}
The issue of covariance estimation is intertwined with random matrix theory, since sample covariance is a particular type of random matrix. These slides follow the structure of chapter 6 of \citet{wainwright2019high} to shed light on random matrices in a \textcolor{red}{non-asymptotic setting}, with the aim of \textcolor{red}{obtaining explicit deviation inequalities that hold for all sample sizes and matrix dimensions.}
{\vskip 0.5em}
In the classical framework of covariance matrix estimation the sample size $n$ tends to infinity while the matrix dimension $d$ is fixed; in this setting the behaviour of sample covariance matrix is characterized by the usual limit theory. In contrast, in high-dimensional settings the data dimension is either comparable to the sample size $(d\asymp n)$ or possibly much larger than the sample size $d\gg n$.
{\vskip0.5em}
We begin with the simplest case, namely ensembles of Gaussian random matrices, and we then discuss more general sub-Gaussian ensembles, before moving to milder tail conditions.  
\end{frame}

\section{Preliminaries}
\subsection{Notations in linear algebra}
\frame{\tableofcontents[currentsection]}



%------------------------------------------------
\begin{frame}[allowframebreaks]
First, let us consider \textcolor{red}{rectangular matrices}, for instance matrix $A\in\R^{n\times m}$ with $n\geq m$, the ordered singular values are written as follows
\[
\sigma_{\max}(A)=\sigma_1(A)\geq\sigma_{2}(A)\geq\cdots\geq\sigma_m(A)=\sigma_{\min}(A)\geq 0
\]
The maximum and minimum singular values are obtained by maximizing the \textquotedblleft blow-up factor\textquotedblright
\[
\sigma_{\max}(A)=\max_{\forall x}\frac{\lVert Ax \rVert_2}{\lVert x\rVert_2},\quad \sigma_{\min}(A)=\min_{\forall x}\frac{\lVert Ax \rVert_2}{\lVert x\rVert_2}
\]
which is obtained when $x$ is the largest and smallest singular vectors respectively - i.e.
\[
\sigma_{\max}(A)=\max_{v\in S^{m-1}}\frac{\lVert Av \rVert_2}{\lVert v\rVert_2},\quad \sigma_{\min}(A)=\min_{v\in S^{m-1}}\frac{\lVert Av \rVert_2}{\lVert v\rVert_2}
\]
noting that $\lVert v\rVert_2=1$, since $S^{d-1}:=\{v\in\R^d\mid \lVert v\rVert_2=1\}$ is the Euclidean unit sphere in $\R^d$. We may denote
\[
\vertiii{A}_2=\sigma_{\max}(A)
\]
However, \textcolor{red}{covariance matrices are square symmetric matrices}, thus we must also focus on symmetric matrices in $\R^d$, denoted $S^{d\times d}:=\{Q\in\R^{d\times d}\mid Q=Q'\}$, as well as subset of semi-definite matrices given by
\[
S_{+}^{d\times d}:=\{Q\in S^{d\times d}\mid Q\geq 0\}.
\]
Any matrix $Q\in S^{d\times d}$ is diagonalizable via unitary transformation, and let us denote the vector of eigenvalues of $Q$ by $\gamma(Q)\in\R^d$ ordered as 
\[
\gamma_{\max}(Q)=\gamma_1(Q)\geq \gamma_2(Q) \geq\cdots\geq\gamma_d(Q)=\gamma_{\min}(Q)
\]
Note the matrix $Q$ is semi-positive definite, which may be expressed as $Q\geq 0$, iff $\gamma_{\min}(Q)\geq 0$.

The Rayleigh-Ritz variational characterization of the minimum and maximum eigenvalues 
\[
\gamma_{\max}(Q)=\max_{v\in S^{d-1}}v'Qv\quad\text{and}\quad\gamma_{\min}(Q)=\min_{v\in S^{d-1}}v'Qv
\]
For symmetric matrix $Q$, the $l_2$ norm can be expressed as 
\[
\vertiii{Q}_2=\max\{\gamma_{\max}(Q), \lvert\gamma_{\min}(Q)\rvert\}:=\max_{v\in S^{d-1}}\lvert v'Qv\rvert
\]
Finally, suppose we have a rectangular matrix $A\in\R^{n\times m}$, with $n\geq m$. We know that any rectangular matrix can be expressed using singular value decomposition (SVD hereafter), as follows 
\[
A=U\Sigma V'
\]
wher $U$ is an $n\times n$ unitary matrix, $\Sigma$ is an $n\times m$ rectangular diagonal matrix with non-negative real numbers on the diagonal up and $V$ is an $n\times n$ unitary matrix. Using SVD, we can express $A'A$ where
\[
A'A=V\Sigma' U'U\Sigma V'
\]
and since $U$ is an orthogonal matrix, we know that $U'U=I$ where $I$ is the identity matrix.
\[
A'A=V(\Sigma'\Sigma) V'
\]
Therefore, as the diagonal matrix $\Sigma$ contains the eigenvalues of matrix $A$, hence, $\Sigma'\Sigma$ contains the eigenvalues of $A'A$ and it can be thus concluded
\[
\gamma_j(A'A)=(\sigma_j(A))^2,\quad j=1,\cdots,m
\]
\end{frame}

\subsection{Set-up of covariance estimation}
\frame{\tableofcontents[currentsection]}

\begin{frame}[allowframebreaks]
Let $\{x_1,\cdots,x_n\}$ be a collection of $n$ i.i.d samples from a distribution in $\R^d$ with zero mean and the covariance matrix $\Sigma$. A standard estimator of sample covariance matrix is
\[
\hat{\Sigma}:=\frac{1}{n}\sum\limits_{i=1}^{n}x_ix_i'.
\]
Since, each $x_i$ for $i=1,\cdots,n$ has zero mean, it is guaranteed that
\[
\E[x_ix_i']=\Sigma
\]
and the random matrix $\hat{\Sigma}$ is an \textcolor{red}{unbiased} estimator of the population covariance $\Sigma$. Consequently the error matrix $\hat{\Sigma}-\Sigma$ has mean zero, and \textcolor{red}{goal is to obtain bounds on the error measures in $l_2$-norm}. We are essentially seeking a band of the form
\[
\vertiii{\hat{\Sigma}-\Sigma}\leq\varepsilon,
\]
where as before, 
\end{frame}
\section{Wishart matrices and their behaviour}
\frame{\tableofcontents[currentsection]}


\section{Covariance matrices from sub-Gaussian ensembles}
\frame{\tableofcontents[currentsection]}

\section{Bounds for general matrices}
\subsection{Background on matrix analysis}
\frame{\tableofcontents[currentsection,currentsubsection]}


\subsection{Tail conditions for matrices}
\frame{\tableofcontents[currentsection,currentsubsection]}


\subsection{Matrix Chernoff approach and independent decompositions}
\frame{\tableofcontents[currentsection,currentsubsection]}


\subsection{Upper tail bounds for random matrices}
\frame{\tableofcontents[currentsection,currentsubsection]}


\subsection{Consequences for covariance matrices}
\frame{\tableofcontents[currentsection,currentsubsection]}

\section{Bounds for structured covariance matrices}
\subsection{Unknown sparsity and thresholding}
\frame{\tableofcontents[currentsection,currentsubsection]}

\begin{frame}[allowframebreaks]
\frametitle{References}
\bibliographystyle{apa}
\bibliography{References_HDStat}
\end{frame}

\end{document}